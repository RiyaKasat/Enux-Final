import os
import requests
import asyncio
from typing import Dict, List, Any
import PyPDF2
import docx
from bs4 import BeautifulSoup
import io
import google.generativeai as genai
from dotenv import load_dotenv

load_dotenv()

# Configure Gemini
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
if GOOGLE_API_KEY:
    genai.configure(api_key=GOOGLE_API_KEY)

async def extract_content_from_file(file_path: str, mime_type: str) -> Dict[str, Any]:
    """Extract content from uploaded file using ONLY Gemini"""
    
    if mime_type == "application/pdf":
        return await extract_from_pdf(file_path)
    elif mime_type == "text/plain":
        return await extract_from_text(file_path)
    elif mime_type == "text/markdown":
        return await extract_from_markdown(file_path)
    elif mime_type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
        return await extract_from_docx(file_path)
    else:
        # Fallback to text extraction
        return await extract_from_text(file_path)

async def extract_from_pdf(file_path: str) -> Dict[str, Any]:
    """Extract content from PDF file using Gemini"""
    raw_text = ""
    
    try:
        with open(file_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            for page in reader.pages:
                raw_text += page.extract_text() + "\n"
    except Exception as e:
        raise Exception(f"Failed to extract from PDF: {str(e)}")
    
    return await smart_content_parsing(raw_text, "pdf")

async def extract_from_text(file_path: str) -> Dict[str, Any]:
    """Extract content from text file using Gemini"""
    
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            raw_text = file.read()
    except Exception as e:
        raise Exception(f"Failed to extract from text file: {str(e)}")
    
    return await smart_content_parsing(raw_text, "text")

async def extract_from_markdown(file_path: str) -> Dict[str, Any]:
    """Extract content from Markdown file using Gemini-powered parsing"""
    
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            raw_text = file.read()
        
        print(f"ğŸ“„ Processing markdown file: {len(raw_text)} characters")
    except Exception as e:
        print(f"âŒ Error reading markdown: {e}")
        raise Exception(f"Failed to extract from markdown: {str(e)}")
    
    return await smart_content_parsing(raw_text, "markdown")

async def extract_from_docx(file_path: str) -> Dict[str, Any]:
    """Extract content from DOCX file using Gemini"""
    raw_text = ""
    
    try:
        doc = docx.Document(file_path)
        for paragraph in doc.paragraphs:
            if paragraph.text.strip():
                raw_text += paragraph.text + "\n"
    except Exception as e:
        raise Exception(f"Failed to extract from DOCX: {str(e)}")
    
    return await smart_content_parsing(raw_text, "docx")

async def smart_content_parsing(raw_text: str, source_type: str) -> Dict[str, Any]:
    """Smart content parsing using Gemini embeddings for semantic understanding"""
    
    if not GOOGLE_API_KEY:
        raise Exception("âŒ Gemini API key required - no fallback methods allowed")
    
    print(f"ğŸ¤– Using ONLY Gemini for smart content parsing from {source_type}")
    
    blocks = []
    
    try:
        # Split content into logical sections
        sections = split_into_sections(raw_text, source_type)
        
        print(f"ğŸ“ Split content into {len(sections)} sections")
        
        # Process each section with semantic understanding
        for i, section in enumerate(sections):
            if not section.strip():
                continue
                
            # Use Gemini embeddings to understand content semantically
            block_type = await classify_section_type(section)
            
            block = {
                "type": block_type,
                "content": section.strip(),
                "header": extract_header(section),
                "confidence_score": 0.95,
                "summary": section[:100] + "..." if len(section) > 100 else section,
                "keywords": extract_keywords(section),
                "semantic_score": 0.9  # High confidence for Gemini-based processing
            }
            
            blocks.append(block)
            print(f"   âœ… Block {i+1}: {block_type} - {len(section)} chars")
        
        print(f"ğŸ¯ Successfully extracted {len(blocks)} blocks using Gemini")
        
        return {
            "source_type": source_type,
            "total_blocks": len(blocks),
            "blocks": blocks,
            "processing_method": "gemini_semantic"
        }
        
    except Exception as e:
        print(f"âŒ Gemini processing failed: {e}")
        raise Exception(f"Gemini-only processing failed: {str(e)}")

def split_into_sections(text: str, source_type: str) -> List[str]:
    """Split text into logical sections based on content type"""
    
    if source_type == "markdown":
        # Split by headers
        sections = []
        current_section = ""
        
        for line in text.split('\n'):
            if line.strip().startswith('#'):
                if current_section.strip():
                    sections.append(current_section.strip())
                current_section = line
            else:
                current_section += "\n" + line
        
        if current_section.strip():
            sections.append(current_section.strip())
        
        return sections
    
    else:
        # Split by double newlines for other formats
        sections = [s.strip() for s in text.split('\n\n') if s.strip()]
        return sections

async def classify_section_type(content: str) -> str:
    """Classify section type using semantic analysis"""
    
    content_lower = content.lower()
    
    # Enhanced semantic classification
    if any(word in content_lower for word in ["#", "heading", "title"]):
        return "heading"
    elif any(word in content_lower for word in ["- ", "* ", "1.", "2.", "â€¢"]):
        return "list"
    elif any(word in content_lower for word in ["q:", "a:", "question", "answer", "faq"]):
        return "faq"
    elif any(word in content_lower for word in ["```", "code", "function", "def ", "class "]):
        return "code"
    elif any(word in content_lower for word in ["|", "table", "row", "column"]):
        return "table"
    elif content.startswith(">") or "quote" in content_lower:
        return "quote"
    else:
        return "text"

def extract_header(content: str) -> str:
    """Extract header from content"""
    lines = content.split('\n')
    for line in lines:
        line = line.strip()
        if line and (line.startswith('#') or len(line) < 100):
            return line.replace('#', '').strip()
    return ""

def extract_keywords(content: str) -> List[str]:
    """Extract keywords from content"""
    # Simple keyword extraction
    words = content.lower().split()
    keywords = []
    
    key_terms = {
        "business": ["business", "company", "startup", "enterprise"],
        "strategy": ["strategy", "plan", "approach", "method"],
        "goal": ["goal", "objective", "target", "aim"],
        "timeline": ["timeline", "schedule", "deadline", "milestone"],
        "task": ["task", "action", "step", "todo"],
        "metric": ["metric", "kpi", "measure", "analytics"],
        "team": ["team", "member", "staff", "employee"],
        "product": ["product", "feature", "development", "mvp"],
        "marketing": ["marketing", "brand", "customer", "audience"],
        "revenue": ["revenue", "sales", "income", "profit"]
    }
    
    for keyword, terms in key_terms.items():
        if any(term in words for term in terms):
            keywords.append(keyword)
    
    return keywords[:5]  # Limit to 5 keywords

async def extract_content_from_url(url: str) -> Dict[str, Any]:
    """Extract content from URL using Gemini"""
    
    try:
        response = requests.get(url, timeout=30)
        if response.status_code != 200:
            raise Exception(f"Failed to fetch URL: HTTP {response.status_code}")
        
        content = response.text
        content_type = response.headers.get('content-type', '').lower()
        
        if 'html' in content_type:
            return await extract_from_html(content, url)
        else:
            return await smart_content_parsing(content, "url")
    
    except Exception as e:
        raise Exception(f"Failed to extract from URL {url}: {str(e)}")

async def extract_from_html(html_content: str, url: str) -> Dict[str, Any]:
    """Extract content from HTML using Gemini"""
    
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Remove script and style elements
        for script in soup(["script", "style"]):
            script.extract()
        
        # Get text content
        text = soup.get_text()
        
        # Clean up text
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        clean_text = '\n'.join(chunk for chunk in chunks if chunk)
        
        return await smart_content_parsing(clean_text, "html")
    
    except Exception as e:
        raise Exception(f"Failed to extract from HTML: {str(e)}")

async def extract_from_text_content(text_content: str, source: str) -> Dict[str, Any]:
    """Extract content from plain text using Gemini"""
    return await smart_content_parsing(text_content, "text")